{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "twtHvjIruiSm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_EvalLLM/blob/main/GEM24_EvalLLM_aiXplain_SM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Aixplain\n",
        "from IPython.display import clear_output\n",
        "! pip install aixplain\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "BWaHAvGj8gCb",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Load human-eval-packaged json, and format contents (triples, text, id)\n",
        "import json\n",
        "import codecs\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "\n",
        "language = \"Spanish\" #@param[\"English\", \"Spanish\", \"Swahili\"]\n",
        "data = 'regular' #@param[\"regular\", \"iaa\"]\n",
        "\n",
        "def format_json(json_path):\n",
        "  # Open en_regular and parse json\n",
        "  en_regular_json = json.load(codecs.open(json_path, 'r', 'utf-8'))\n",
        "  # Print first entry\n",
        "  # print(json.dumps(en_regular_json[0], indent=4))\n",
        "\n",
        "  triples_text_pairs = []\n",
        "\n",
        "  x = 0\n",
        "  while x < len(en_regular_json):\n",
        "    # if x < 10:\n",
        "    # Parse html found in the \"input\" key\n",
        "    html = en_regular_json[x]['input']\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    # Print raw table\n",
        "    # print(soup.prettify())\n",
        "    table = soup.find('table')\n",
        "    # headers = [header.text.strip() for header in table.find_all('th')]\n",
        "    rows = []\n",
        "    for row in table.find_all('tr'):\n",
        "      columns = row.find_all(['td', 'th'])  # Get both <td> and <th>\n",
        "      row_data = ' '.join([col.text.strip() for col in columns])\n",
        "      rows.append(row_data)\n",
        "    triples_formatted = '; '.join(rows[1:]) # exclude header\n",
        "    # print(\"Headers:\", rows[0])\n",
        "    # print(rows[1:])\n",
        "    triples_text_pairs.append({'id':en_regular_json[x]['id'], 'triples': '\"\"\"'+triples_formatted+'\"\"\"', 'text': en_regular_json[x]['output']})\n",
        "    # else:\n",
        "    #   break\n",
        "    x += 1\n",
        "  return triples_text_pairs\n",
        "\n",
        "if language == \"English\":\n",
        "  if data == 'regular':\n",
        "    if not os.path.exists('en_regular.json'):\n",
        "      ! gdown 1hOp1_zN2IgGeTnmzngOgIdLRXErchkEo\n",
        "    triples_text_pairs = format_json('en_regular.json')\n",
        "  elif data == 'iaa':\n",
        "    if not os.path.exists('en_iaa.json'):\n",
        "      ! gdown 1Q69osoEzJJBVqYH9t2ArPAaPAHVPNax5\n",
        "    triples_text_pairs = format_json('en_iaa.json')\n",
        "\n",
        "elif language == \"Spanish\":\n",
        "  if data == 'regular':\n",
        "    if not os.path.exists('es_regular.json'):\n",
        "      ! gdown 14hNTAfMqQI2-K81O9GZ4GkoybI2iO-sh\n",
        "    triples_text_pairs = format_json('es_regular.json')\n",
        "  elif data == 'iaa':\n",
        "    if not os.path.exists('es_iaa.json'):\n",
        "      ! gdown 1HadgIdWzcb7NTJYELKB-MJ-qa510e3M4\n",
        "    triples_text_pairs = format_json('es_iaa.json')\n",
        "\n",
        "elif language == \"Swahili\":\n",
        "  if data == 'regular':\n",
        "    if not os.path.exists('sw_regular.json'):\n",
        "      ! gdown 1Knp231CEdUQ-SDnJ_oknJDm2dogetSFg\n",
        "    triples_text_pairs = format_json('sw_regular.json')\n",
        "  elif data == 'iaa':\n",
        "    if not os.path.exists('sw_iaa.json'):\n",
        "      ! gdown 16JFZg8DacuhRvIAUJ0iMVqnvh4nrxZTX\n",
        "    triples_text_pairs = format_json('sw_iaa.json')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fKegUHmi3u52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print sample datapoint\n",
        "expected = 0\n",
        "if language == \"English\":\n",
        "  if data == 'regular':\n",
        "    expected = 8280\n",
        "    print('Expected: 8280')\n",
        "  elif data == 'iaa':\n",
        "    expected = 92\n",
        "    print('Expected: 92')\n",
        "elif language == \"Spanish\":\n",
        "  if data == 'regular':\n",
        "    expected = 3240\n",
        "    print('Expected: 3240')\n",
        "  elif data == 'iaa':\n",
        "    expected = 72\n",
        "    print('Expected: 72')\n",
        "elif language == \"Swahili\":\n",
        "  if data == 'regular':\n",
        "    expected = 2700\n",
        "    print('Expected: 2700')\n",
        "  elif data == 'iaa':\n",
        "    expected = 60\n",
        "    print('Expected: 60')\n",
        "print(f'Found: {len(triples_text_pairs)}')\n",
        "\n",
        "if not len(triples_text_pairs) == expected:\n",
        "  print('--------------------------------------------------\\n!!! ERROR: mismatch expected/found data points !!!\\n--------------------------------------------------')\n",
        "\n",
        "print(triples_text_pairs[0])\n",
        "\n",
        "# Triples = '\"\"\"Marcus_Aurelius HasChild Fadilla; Marcus_Aurelius StudentOf Alexander_of_Cotiaeum; Marcus_Aurelius Spouse Faustina_the_Younger; Marcus_Aurelius PositionHeld Roman_emperor\"\"\"'\n",
        "# Nice_Text = '''Marcus Aurelius was a student of Alexander of Cotiaeum and had a child named Fadilla. He was married to Faustina the Younger. He was a Roman emperor and died in Vindobona.'''"
      ],
      "metadata": {
        "id": "cxg80KZZCNyY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofwSLwpTyK9U",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run evaluation (needs aiXplain API key in Parameters)\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "# PARAMETERS aiXplain\n",
        "#==========================\n",
        "os.environ[\"AIXPLAIN_API_KEY\"] = 'insertYourKeyHere'\n",
        "Gemini_1dot5_Flash = '674b73f06eb563a748561d41' # Code double checked\n",
        "path_out = 'Gemini_results'\n",
        "#==========================\n",
        "from aixplain.factories import AgentFactory\n",
        "# from aixplain.modules.agent import ModelTool\n",
        "from aixplain.modules.agent.tool.model_tool import ModelTool\n",
        "\n",
        "if not os.path.exists(path_out):\n",
        "  os.makedirs(path_out)\n",
        "\n",
        "def dumpResults(annotations, path_out):\n",
        "  results_file = open(os.path.join(path_out, 'All_Gemini_results'), 'ab')\n",
        "  pickle.dump(annotations, results_file)\n",
        "  results_file.close()\n",
        "\n",
        "def callGemini_aiXplain(prompt, model):\n",
        "  agent = AgentFactory.create(\n",
        "    name=\"Assessment of text quality\",\n",
        "\t  description=\"Assessment of text quality\",\n",
        "\t  instructions=\"\",\n",
        "    tools=[\n",
        "      ModelTool(model=model),\n",
        "    ],\n",
        "  )\n",
        "  agent_response = agent.run(prompt)\n",
        "\n",
        "  return agent_response\n",
        "\n",
        "def runEval(triples_text_pairs, model):\n",
        "  # EN regular splits: range(0, 2750), range(2750, 5500), range(5500, 8240)\n",
        "  x = 0\n",
        "  # To get all evaluations\n",
        "  for x in range(0, len(triples_text_pairs)):\n",
        "  # To test on a few inputs only\n",
        "  # while x < 2:\n",
        "    Triples = triples_text_pairs[x]['triples']\n",
        "    Nice_Text = triples_text_pairs[x]['text']\n",
        "    id = triples_text_pairs[x]['id']\n",
        "\n",
        "    #Prompt (Do not change unless discussed with the GEM-HumEval group)\n",
        "    prompt = '''\n",
        "In this task, you will evaluate the quality of the Text in relation to the given Triple Set. How well does the Text represent the Triple Set?  You will be given four specific Dimensions to evaluate against:\n",
        "\n",
        "Dimensions:\"\"\"\n",
        "No-Omissions: ALL the information in the Triple Set is present in the Text.\n",
        "No-Additions: ONLY information from the Triple Set is present in the Text.\n",
        "Grammaticality: The Text is free of grammatical and spelling errors.\n",
        "Fluency: The Text flows well and is easy to read; its parts are connected in a natural way.\"\"\"\n",
        "\n",
        "Important note on No-Omissions and No-Additions: some Triple Set/Text pairs contain non-factual information and even fictional names for people, places, dates, etc. Whether there are omissions and/or additions in a Text is NOT related to factual truth, but instead is strictly related to the contents of the input Triple Set.\n",
        "Important note on Grammaticality and Fluency: for Grammaticality and Fluency you do not need to consider the input Triple Set; only the intrinsic quality of the Text needs to be assessed.\n",
        "\n",
        "You need to provide the scores ranging from 1 (indicating the lowest score) to 7 (indicating the highest score) for each of the dimensions and a short justification for each score in the following JSON format:  {\"No-Omissions\": {\"Justification\": \"\", \"Score\": \"\"}, \"No-Additions\": {\"Justification\": \"\", \"Score\": \"\"}, \"Grammaticality\": {\"Justification\": \"\", \"Score\": \"\"}, \"Fluency\": {\"Justification\": \"\", \"Score\": \"\"} }.\n",
        "\n",
        "Make sure to read thoroughly the Triple Set and the '''+str(language)+''' Text below, and assess the four Dimensions using the instructions and template above.\n",
        "\n",
        "Triple Set: ''' + str(Triples) + \"\\n\" + '''Text: '''+ str(Nice_Text) + \"\\n\\n\" + '''\n",
        "'''\n",
        "    print(f'Evaluating text #{x}...')\n",
        "\n",
        "    responseGemini = callGemini_aiXplain(prompt, model)\n",
        "    # Gemini outputs single quotes but json.loads expects double quotes\n",
        "    out_json = json.loads(str(responseGemini['data']['output']).replace(\"'\", '\"'))\n",
        "    # Gemini adds a node \"query\" in the json, which I remove so the outputs are the same as OpenAI's\n",
        "    triples_text_pairs[x]['scores_Gemini'] = out_json['query']\n",
        "\n",
        "    # Save individual files as backup\n",
        "    with open(os.path.join('Gemini_results', 'Gemini_results_'+str(id)), 'ab') as f:\n",
        "      pickle.dump(triples_text_pairs[x], f)\n",
        "\n",
        "    x += 1\n",
        "\n",
        "  return triples_text_pairs\n",
        "\n",
        "annotations = runEval(triples_text_pairs, Gemini_1dot5_Flash)\n",
        "\n",
        "# dumpResults(annotations, path_out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download GPT_results folder\n",
        "from google.colab import files\n",
        "import datetime\n",
        "\n",
        "date = datetime.date.today().strftime(\"%y%m%d\")\n",
        "zip_name = f'{date}_{language}_GeminiFlash1dot5_{data}_{str(len(triples_text_pairs))}.zip'\n",
        "# zip_name = f'{date}_{language}_{model}_{data}_0001-2750.zip'\n",
        "\n",
        "!zip -r /content/{zip_name} /content/Gemini_results\n",
        "files.download(zip_name)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qv63xq_QO_6d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results analysis"
      ],
      "metadata": {
        "id": "twtHvjIruiSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip uploaded file\n",
        "!unzip /content/GPT_results.zip -d /content"
      ],
      "metadata": {
        "id": "ajyFBxlFPWaD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load file\n",
        "import pickle\n",
        "import glob\n",
        "import json\n",
        "\n",
        "def loadAllData(dbfile):\n",
        "  # load data with pickle\n",
        "  datapoints = pickle.load(dbfile)\n",
        "  for dp in datapoints:\n",
        "    if 'scores_Gemini' in dp:\n",
        "      print(dp['id'])\n",
        "\n",
        "# dbfile_all = open('/content/content/GPT_results/All_GPT_results', 'rb')\n",
        "# loadAllData(dbfile_all)\n",
        "\n",
        "print('----------------')\n",
        "\n",
        "def loadDataPoint(dbfile_x):\n",
        "  # load data with pickle\n",
        "  dp = pickle.load(dbfile_x)\n",
        "  if 'scores_Gemini' in dp:\n",
        "    print(dp['id'])\n",
        "    print(dp['triples'])\n",
        "    print(dp['text'])\n",
        "    print(dp['scores_Gemini'])\n",
        "    # pickle.load also uses single quotes, whereas json.load expects double quotes\n",
        "    scores_json = json.loads(str(dp['scores_Gemini']).replace(\"'\", '\"'))\n",
        "    print(f\"Gram: {scores_json['Grammaticality']['Score']}; Flu: {scores_json['Fluency']['Score']}; NoOm: {scores_json['No-Omissions']['Score']}; NoAd: {scores_json['No-Additions']['Score']}.\")\n",
        "    print('')\n",
        "\n",
        "eval_files = glob.glob('Gemini_results/Gemini_results_*')\n",
        "for filepath in eval_files:\n",
        "  print(filepath)\n",
        "  dbfile_x = open(filepath, 'rb')\n",
        "  loadDataPoint(dbfile_x)"
      ],
      "metadata": {
        "id": "cQDPBbTqIVOW",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}